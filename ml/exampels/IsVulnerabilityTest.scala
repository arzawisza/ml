package com.azawisza.logs

import java.io.{BufferedWriter, File, FileWriter, PrintWriter}

import com.sundogsoftware.sparkstreaming.Record
import nl.basjes.parse.httpdlog.HttpdLoglineParser
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.{Duration, Seconds, StreamingContext}
import org.scalatest.{BeforeAndAfterEach, FlatSpec}

import scala.collection.{immutable, mutable}

class IsVulnerabilityTest extends FlatSpec with BeforeAndAfterEach {

  var context: StreamingContext = _
  val ipr = """.*(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3}).*""".r

  override def beforeEach(): Unit = {
    context = new StreamingContext("local[4]", "App",Duration(100))
  }

  "LFI "should "be recognized in access files" in {
      //year logs, 2 types
      //dict and rgex
      //files aggregated on vulnerability
      //all log files in sequence of occurences

      //stream of RDD[FileAsText]
      //
      //

    val accessFormat = "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\""
    val accessParser = new HttpdLoglineParser(classOf[Record], accessFormat)

    val queue = mutable.Queue[RDD[String]]()
    val stream: InputDStream[String] = context.queueStream(queue)
    queue++= dirAsRDDTextFilesContent(context, "")
    stream.foreachRDD(rdd => {
      rdd.foreach(fileContent => {
        fileContent.lines
          .flatMap(_.lines)
          .map(accessParser.parse)
          .foreach(println)
    })})
  }


  "LFI "should "be recognized in error files" in {
    //year logs, 2 types
    //dict and rgex
    //files aggregated on vulnerability
    //all log files in sequence of occurences

    //stream of RDD[FileAsText]
    //
    //
    //val errorFormat = """^(\[[^\]]+\]) (\[[^\]]+\]) (\[[^\]]+\]) (.*)$""".r



    checkLfi2016()
    context.start()
    context.awaitTermination()
  }

  private def checkLfi2016() = {

    val lfi:Set[String] = scala.io.Source.fromFile("src/test/resources/hacks/dict/lfi/fuzzdb-lfi.txt").mkString.lines.toSet
    val queue = mutable.Queue[RDD[String]]()
    val stream: InputDStream[String] = context.queueStream(queue)
    queue ++= dirAsRDDTextFilesContent(context, "src/test/resources/hacks/2016", isErrorFile = true)
    val broadcast:Broadcast[Set[String]] = context.sparkContext.broadcast(lfi)
    val file: File = new File(s"lfi-2016.txt")

    val accum = context.sparkContext.accumulator(0, "Accumulator Example")
    stream.foreachRDD(rdd => {
      println("------------>"+accum .value)
      rdd.foreach(fileContent => {
        println("------------>"+fileContent)
        accum+=1
        val pw = new BufferedWriter(new FileWriter(file, true))
        fileContent.lines
          .flatMap(_.lines)
          /*.map(w => {
            val strings:Array[String] = errorFormat.split(w)
            val size = strings.size
            strings
          })*/
          .filter(e => Filter .isLfiThreat(broadcast.value, e))
          .foreach(k =>{

            pw.append(k+"\n")
            pw.flush()
          })
      })

    })
  }



  def dirAsRDDTextFilesContent(ssc: StreamingContext, path:String, isErrorFile:Boolean= false): Seq[RDD[String]] = {
    val files = readFilesOf(path)//"src/test/resources/data"
    val seq:immutable.Seq[RDD[String]] = files
      .filter(_.getName.contains("error")== isErrorFile)
      .map(f => {
        val string = scala.io.Source.fromFile(f).mkString
        string
      })
      .map(txt => context.sparkContext.makeRDD(Seq(txt)))
      .toSeq
    seq
  }



  def readFilesOf(path: String): List[File] = {
    val d = new File(path)
    if (d.exists && d.isDirectory) {
      d.listFiles.filter(_.isFile).toList
    } else {
      List()
    }
  }


  override def afterEach(): Unit = {
    context.stop()
  }

}
